{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f138d1d5-efb3-4896-9153-7aa35ed6c27b",
   "metadata": {},
   "source": [
    "# Exercise session 7: SQP\n",
    "\n",
    "Before we start the exercise session, let's define some useful functions we will be using later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629826b9-6cc2-4cbc-86cd-a2eae2464f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import casadi as cs\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d58bc3-6a79-4a42-9fbe-a1654e13c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iterates(x: np.array):\n",
    "    plt.figure()\n",
    "    plt.xlabel('x-axis')\n",
    "    plt.ylabel('y-axis')\n",
    "    for i in range(x.size(1)):\n",
    "        plt.plot(x[0,i], x[1,i],color='r')\n",
    "        plt.plot(x[0,i], x[1,i],color='r', marker=\"o\")\n",
    "        plt.show()\n",
    "        pause(0.1)\n",
    "\n",
    "def plot_convergence(n_iter, gradients_lagrangian):\n",
    "    plt.figure()\n",
    "    plt.semilogy(np.arange(n_iter), gradients_lagrangian.squeeze(), marker='.')\n",
    "    plt.grid()\n",
    "    plt.title('Gradient norm iterations')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('$\\\\Vert \\\\nabla L(x_k, \\\\lambda_k)\\\\Vert_\\\\infty$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eb6dd3-6ef9-46c1-8602-00fec6adea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_difference_jacobian(fun, x0: np.array):\n",
    "    # make sure x0 is a column vector\n",
    "    Nx,cols = x0.shape\n",
    "    if cols != 1:\n",
    "        raise ValueError('x0 needs to be a column vector')\n",
    "\n",
    "    # make sure fun returns a column vector\n",
    "    f0 = fun(x0)\n",
    "    Nf,cols = f0.shape\n",
    "    if cols != 1:\n",
    "        raise ValueError('fun needs to return a column vector');\n",
    "\n",
    "    # initialize empty J\n",
    "    J = np.zeros((Nf, Nx))\n",
    "\n",
    "    # perform the finite difference jacobian evaluation\n",
    "    h = 1e-6\n",
    "    for k in range(Nx):\n",
    "        x = x0.copy()\n",
    "        x[k] = x[k] + h\n",
    "        f = fun(x)\n",
    "        grad = (f - f0)/h\n",
    "        J[:,[k]] = grad\n",
    "\n",
    "    return f0, J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086e3723-6fd2-4dea-bbe8-8e22c9762f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_search(fun, x0, Jdk, dk, sigma, beta):\n",
    "    # Line search using Armijo conditions and backtracking\n",
    "    # fun   - scalar function\n",
    "    # x0    - initial guess\n",
    "    # Jdk   - J is gradient of fun at x0, Jdk is J'*dk\n",
    "    # dk    - search direction\n",
    "    # sigma - Armijo condition scaling function\n",
    "    # beta  - backtracking parameter\n",
    "\n",
    "    # make sure sigma and beta are in reasonable range\n",
    "    if sigma <= 0 or 1 <= sigma:\n",
    "        raise ValueError('sigma must be in (0,1)')\n",
    "\n",
    "    if beta <= 0 or 1 <= beta:\n",
    "        raise ValueError('beta must be in (0,1)')\n",
    "    \n",
    "    too_many_steps_counter = 1000\n",
    "\n",
    "    # initialize alpha, evaluate function\n",
    "    alpha = 1\n",
    "    f0 = fun(x0)\n",
    "\n",
    "    trial_x = x0 + alpha*dk\n",
    "    while fun(trial_x) > f0 + sigma*alpha*Jdk:\n",
    "        # trial step in x\n",
    "        alpha   = beta * alpha\n",
    "        trial_x = x0 + alpha*dk\n",
    "\n",
    "        # if the line search takes too many iterations, throw an error\n",
    "        too_many_steps_counter = too_many_steps_counter - 1\n",
    "        if too_many_steps_counter == 0:\n",
    "            raise RuntimeError('line search fail - took too many iterations')\n",
    "\n",
    "    return trial_x, alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56374a6-65ee-4198-81ab-afba0f771247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merit_function_equality_constrained_sqp(ffun, hfun, c, x):\n",
    "    f = ffun(x)\n",
    "    h = hfun(x)    \n",
    "    ret = f + c*np.linalg.norm(h, 1) #Calculates the 1-norm\n",
    "    return ret\n",
    "\n",
    "def merit_function_inequality_constrained_sqp(ffun, hfun, gfun, c, x):\n",
    "\n",
    "    f = ffun(x)    \n",
    "    h = hfun(x)\n",
    "    g = gfun(x)\n",
    "    \n",
    "    ret = f + c* (np.linalg.norm(h, 1) + np.linalg.norm(np.fmax(g,0),1))\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccef3c3f-8433-43ff-a1b0-cb8853138866",
   "metadata": {},
   "source": [
    "$\\renewcommand\\R{\\mathrm{I\\!R}}$\n",
    "## Exercise 1\n",
    "Consider the problem:\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "\\min_{x\\in\\R^n} & \\quad f(x) \\\\\n",
    "\\text{s.t.} & \\quad h(x) = 0\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "with $f\\colon\\R^n\\to\\R$, $h\\colon\\R^n\\to\\R^m$.\n",
    "\n",
    "**Newton Lagrange Method**: We use Lagrange multipliers $\\lambda\\in\\R^m$ to handle the equality constraints:\n",
    "\\begin{align*}\n",
    "{L}(x,\\lambda) = f(x) + \\lambda^Th(x).\n",
    "\\end{align*}\n",
    "The optimality conditions are given by:\n",
    "\\begin{align*}\n",
    " 0 = \\nabla_x{L}(x,\\lambda) &= \\nabla_x f(x) + \\nabla_x h(x) \\lambda\\\\\n",
    " 0 = \\nabla_\\lambda {L}(x,\\lambda) &= h(x).\n",
    "\\end{align*}\n",
    "Applying Newton's method on the KKT system yields for a given $x_k$ and $\\lambda_k$:\n",
    "\\begin{align}\n",
    "  \\begin{pmatrix}\n",
    "    \\nabla_{xx}^2{L}_k & \\nabla h_k \\\\\n",
    "    \\nabla h_k^T & 0 \\\\\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "    \\Delta x \\\\\n",
    "    \\lambda\n",
    "  \\end{pmatrix}\n",
    "  =\n",
    "  \\begin{pmatrix}\n",
    "    -\\nabla f_k \\\\\n",
    "    -h_k\n",
    "  \\end{pmatrix}.\n",
    "\\end{align}\n",
    "By solving this linear system, we obtained $\\Delta x$ and $\\lambda$. The next iterate is calculated by\n",
    "\\begin{align*}\n",
    "    &x_{k+1} = x_k + \\Delta x,\\\\\n",
    "    &\\lambda_{k+1} = \\lambda.\n",
    "\\end{align*}\n",
    "\n",
    "**QP Interpretation**: Consider the quadratic program:\n",
    "\\begin{align}\n",
    "\\begin{split}\n",
    "\\min_{\\Delta x} & \\ \\Delta x^T\\nabla f_k + \\frac{1}{2}\\Delta x^T \\nabla_{xx}^2{L}_k \\Delta x \\\\\n",
    "\\text{s.t.} & \\ \\nabla h_k^T \\Delta x + h_k = 0\n",
    "\\end{split}\n",
    "\\end{align}\n",
    "Introduce Lagrange multipliers $\\lambda$:\n",
    "\\begin{align*}\n",
    "{L}(\\Delta x,\\lambda) = \\Delta x^T\\nabla f_k + \\frac{1}{2}\\Delta x^T \\nabla_{xx}^2{L}_k\\Delta x  + \\lambda^T(\\nabla h_k^T \\Delta x + h_k )\n",
    "\\end{align*}\n",
    "and the optimality conditions are:\n",
    "\\begin{align*}\n",
    "\\nabla f_k + \\nabla_{xx}^2 {L}_k \\Delta x + \\nabla h_k \\lambda &= 0\\\\\n",
    "\\nabla h_k^T\\Delta x + h_k &= 0.\n",
    "\\end{align*}\n",
    "Rewriting this as a linear system, we have:\n",
    "\\begin{align}\n",
    "  \\begin{pmatrix}\n",
    "    \\nabla_{xx}^2{L}_k & \\nabla h_k \\\\\n",
    "    \\nabla h_k^T & 0 \\\\\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "    \\Delta x \\\\\n",
    "    \\lambda\n",
    "  \\end{pmatrix}\n",
    "  =\n",
    "  \\begin{pmatrix}\n",
    "    -\\nabla f_k \\\\\n",
    "    -h_k\n",
    "  \\end{pmatrix}.\n",
    "\\end{align}\n",
    "\n",
    "**Since this KKT system is identical to the one we obtained before, we can use a QP solver directly solve instead of solving this linear system ourselves.**\n",
    "\n",
    "### Exercise 1.a\n",
    "Complete the code below to use the QP solver OSQP interfaced through the package Casadi, instead of solving for the linear system of equations. A description of the Casadi conic function can be found in https://web.casadi.org/docs/#quadratic-programming.\n",
    "You will need to get $\\lambda$ from the output of conic.\n",
    "As before, use BFGS to approximate the Hessian $\\nabla_{xx}^2 {L}$.$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5270c241-9779-4633-81fb-bcd95578f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_eq_sqp(ffun, hfun, x0: np.array, with_line_search=True, with_powells_trick=False):\n",
    "\n",
    "    # convergence tolerance\n",
    "    grad_tol = 1e-4\n",
    "    max_iters = 100\n",
    "    \n",
    "    # make sure x0 is a column vector\n",
    "    n_variables, cols = x0.shape\n",
    "    if cols != 1:\n",
    "        raise ValueError('x0 needs to be a column vector')\n",
    "    \n",
    "    # make sure ffun returns a scalar\n",
    "    f0 = ffun(x0)\n",
    "    if len(f0) != 1:\n",
    "        raise ValueError('ffun must return a scalar')\n",
    "    \n",
    "    # make sure hfun returns a column vector\n",
    "    h0 = hfun(x0)\n",
    "    n_eq_constraints, cols = h0.shape\n",
    "    if cols != 1:\n",
    "        raise ValueError('hfun needs to return a column vector')\n",
    "    \n",
    "    # a log of the iterations\n",
    "    x_iters = np.zeros((n_variables, max_iters))\n",
    "    gradient_lagrangian_iters = np.zeros((1,max_iters))\n",
    "    \n",
    "    # initial variables\n",
    "    x = x0.copy()\n",
    "    lambdA = np.zeros((n_eq_constraints, 1)) # lambda is a key word\n",
    "    \n",
    "    # initialize B\n",
    "    B = np.eye(n_variables)\n",
    "    \n",
    "    # evaluate the initial f and it's jacobian J\n",
    "    _, J = finite_difference_jacobian(ffun, x)\n",
    "    \n",
    "    # evaluate the initial h and it's jacobian J_h\n",
    "    h, J_h = finite_difference_jacobian(hfun, x)\n",
    "\n",
    "    # Define the QP solver with Casadi\n",
    "    B_placeholder = cs.DM.ones(n_variables, n_variables)\n",
    "    Jh_placeholder = cs.DM.ones(n_eq_constraints, n_variables)\n",
    "    qp_struct = {   'h': B_placeholder.sparsity(),\n",
    "                    'a': Jh_placeholder.sparsity()}\n",
    "            \n",
    "    qp_solver_opts = {}\n",
    "    qp_solver_opts[\"print_out\"] = False\n",
    "    qp_solver_opts[\"osqp\"] = {\"verbose\": False,\n",
    "                              \"eps_abs\":1e-4,\n",
    "                              \"eps_rel\":1e-4,\n",
    "                              \"eps_prim_inf\":1e-5,\n",
    "                              \"eps_dual_inf\":1e-5,\n",
    "                              \"max_iter\":4000}\n",
    "    \n",
    "    qp_solver = cs.conic(\"qpsol\", \"osqp\", qp_struct, qp_solver_opts)\n",
    "\n",
    "    # Main optimization loop\n",
    "    for k in range(max_iters):\n",
    "        \n",
    "        # check for divergence\n",
    "        x_norm_inf = np.linalg.norm(x, np.inf)\n",
    "        if x_norm_inf > 1e6:\n",
    "            raise ValueError('minimize_sqp has diverged, ||x||_\\{inf\\}: %.3g',x_norm_inf)\n",
    "        \n",
    "        # store x in the iteration log\n",
    "        gradient_lagrangian = J.T + J_h.T  @ lambdA\n",
    "        norm_gradient_lagrangian = np.linalg.norm(gradient_lagrangian, np.inf)\n",
    "        norm_infeasbility = np.linalg.norm(h, np.inf)\n",
    "\n",
    "        norm_convergence = np.fmax(norm_gradient_lagrangian, norm_infeasbility)\n",
    "\n",
    "        x_iters[:,[k]] = x.copy()\n",
    "        gradient_lagrangian_iters[0, k] = norm_convergence\n",
    "        \n",
    "        print('iteration: {},   convergence_metric: {:.2e}'.format(k, norm_convergence))\n",
    "    \n",
    "        # check for convergence\n",
    "        if norm_convergence < grad_tol:\n",
    "            x_iters = x_iters[:,:k+1]\n",
    "            gradient_lagrangian_iters = gradient_lagrangian_iters[:,:k+1]\n",
    "            print('acceptable solution found\\n')\n",
    "            return x, x_iters, gradient_lagrangian_iters\n",
    "        \n",
    "        # lambdA_old = lambdA.copy()\n",
    "        x_old = x.copy()\n",
    "        \n",
    "        #################### FILL THIS PART IN ########################\n",
    "        # find the search direction and lambda\n",
    "\n",
    "        ### OLD SCHOOL WITH SOLVING LINEAR SYSTEM -----------------------------\n",
    "        # M = np.vstack((np.hstack((B, J_h.T)), \n",
    "        #                np.hstack((J_h,  np.zeros((n_eq_constraints, n_eq_constraints))))\n",
    "        #                ))\n",
    "        # L0 = np.vstack((-J.T, -h))\n",
    "        # sol = np.linalg.solve(M, L0)\n",
    "        # dk = sol[:n_variables]\n",
    "        # lambdA = sol[n_variables:]\n",
    "\n",
    "        ### NEW APPROACH SOLVING QP\n",
    "        lba = [-h]\n",
    "        uba = [-h]\n",
    "        result = qp_solver(h=B,\n",
    "                           a=J_h,\n",
    "                           g=J,\n",
    "                           lba=lba,\n",
    "                           uba=uba)\n",
    "\n",
    "        dk = np.array(result['x'])\n",
    "        lambdA = np.array(result['lam_a'])\n",
    "\n",
    "        ### END OF DIFFERENT APPROACHES ---------------------------------------\n",
    "        # take the line search\n",
    "        if with_line_search:\n",
    "            sigma = 0.01\n",
    "            beta  = 0.6\n",
    "            c     = 100\n",
    "            # calculate the directional derivative\n",
    "            directional_derivative = J @ dk - c*np.linalg.norm(h,1)\n",
    "            merit_fun = lambda y : merit_function_equality_constrained_sqp(ffun, hfun, c, y)\n",
    "            x, _ = line_search(merit_fun, x, directional_derivative, dk, sigma, beta)\n",
    "        else:\n",
    "            x += dk\n",
    "\n",
    "        # ------ BFGS Hessian Approximation ------\n",
    "        J_old = J.copy()\n",
    "        J_h_old = J_h.copy()\n",
    "        \n",
    "        # evaluate F and it's jacobian J\n",
    "        _, J = finite_difference_jacobian(ffun, x)\n",
    "    \n",
    "        # evaluate h and it's jacobian J_h\n",
    "        h, J_h = finite_difference_jacobian(hfun, x)\n",
    "        \n",
    "        Lx_old = J_old.T + J_h_old.T @ lambdA\n",
    "        Lx     = J.T     + J_h.T @ lambdA\n",
    "        \n",
    "        s = x - x_old\n",
    "        y = Lx - Lx_old\n",
    "\n",
    "        # Powell's trick\n",
    "        if with_powells_trick:\n",
    "            if y.T @ s >= 0.2 * s.T @ B @ s:\n",
    "                theta = 1\n",
    "            else:\n",
    "                theta = (0.8 * s.T @ B @ s)/(s.T @ B @ s -s.T @ y)\n",
    "\n",
    "            y = theta*y + (1-theta) * B @ s\n",
    "        else:\n",
    "            y = y\n",
    "\n",
    "        B = B - B @ (s@s.T) @ B/(s.T@B@s) + y@y.T/(s.T @ y)\n",
    "\n",
    "        # Symmetrize Hessian (only needed for numerical accuracy)\n",
    "        B = (B + B.T) / 2. # Symmetrize Hessian, only needed due to numerical inaccuracy, so that quadprog doesn't complain\n",
    "        \n",
    "        ###############################################################    \n",
    "\n",
    "    raise RuntimeError('minimize_eq_sqp: max iterations exceeded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ac7409-38da-4e66-9300-04dae869e245",
   "metadata": {},
   "source": [
    "### Exercise 1.b\n",
    "Verify your modification by using your solver on the problem from last time:\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "\\min_{x,y} & \\ \\frac{1}{2}\\left( x^2 + \\left(\\frac{y}{2}\\right)^2 \\right) \\\\\n",
    "\\text{s.t.} & \\ y = (x-1)^2 - x + 3\n",
    "\\end{split}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab70dc2b-db4e-4275-9958-577eee9cb743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function\n",
    "def ffun(xy: np.array):\n",
    "    x = xy[0]\n",
    "    y = xy[1]    \n",
    "    f = 0.5*(y/2)**2 + 0.5*x**2\n",
    "    return np.array([f])\n",
    "\n",
    "# Equality constraint function\n",
    "def hfun(xy: np.array):\n",
    "    x = xy[0]\n",
    "    y = xy[1]    \n",
    "    h = y - (3 + (x-1)**2 - x)\n",
    "    return np.array([h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a0491f-9aad-4335-b0e9-10a5465bc289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex7_eq_sqp(with_line_search=True, with_powells_trick=False, plot_function=False):\n",
    "\n",
    "    # ------ First let's plot the function ------\n",
    "    # Define the the meshgrids for the 2D function\n",
    "    xlist = np.linspace(-1, 4, 40)\n",
    "    ylist = np.linspace(-3, 4, 40)\n",
    "    [X,Y] = np.meshgrid(xlist, ylist)\n",
    "    obj = np.vectorize(lambda x, y: ffun([x, y]), signature=\"(),()->()\")(X, Y)\n",
    "    \n",
    "    if plot_function:\n",
    "        # Plot contour lines\n",
    "        fig = plt.figure()\n",
    "        c = plt.contour(X, Y, obj, 40)\n",
    "        t = np.linspace(-4,4,400)\n",
    "        plt.plot(t, 3 + (t-1)**2 - t, color='black', label='constraint')\n",
    "    \n",
    "        plt.title('Contour plot of objective function with constraint')\n",
    "        plt.xlabel('x [cm]')\n",
    "        plt.ylabel('y [cm]')\n",
    "        plt.xlim((-1, 4))\n",
    "        plt.ylim((-3, 4))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # ------ Solve the problem ------\n",
    "    # Initial guess\n",
    "    x0 = np.array([[3.0],[1.0]])\n",
    "    # x0 = np.array([[2.5], [3.]])\n",
    "\n",
    "    _, x, gradients_lagrangian = minimize_eq_sqp(ffun,\n",
    "                                                 hfun,\n",
    "                                                 x0,\n",
    "                                                 with_line_search,\n",
    "                                                 with_powells_trick)\n",
    "\n",
    "    n_iter = gradients_lagrangian.shape[1]\n",
    "\n",
    "    # ------ Plot the solution ------\n",
    "    # Plot iterates x\n",
    "    # Plot contour lines\n",
    "    fig = plt.figure()\n",
    "    c = plt.contour(X, Y, obj, 40)\n",
    "    t = np.linspace(-4,4,400)\n",
    "    plt.plot(t, 3 + (t-1)**2 - t, color='black', label='constraint')\n",
    "\n",
    "    plt.plot(x[0,:], x[1,:],color='r')\n",
    "    plt.plot(x[0,:], x[1,:],color='r', marker=\"o\")\n",
    "    plt.title('Contour plot of objective function with constraint')\n",
    "    plt.xlabel('x [cm]')\n",
    "    plt.ylabel('y [cm]')\n",
    "    plt.xlim((-1, 4))\n",
    "    plt.ylim((-3, 4))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot convergence\n",
    "    plot_convergence(n_iter, gradients_lagrangian)\n",
    "\n",
    "\n",
    "# Solve the problem\n",
    "ex7_eq_sqp(with_line_search=False, with_powells_trick=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f861021-0b55-41e5-baa9-20aefe04d4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No linesearch, no Powell\n",
    "ex7_eq_sqp(with_line_search=False, with_powells_trick=False)\n",
    "\n",
    "# Only linesearch\n",
    "ex7_eq_sqp(with_line_search=True, with_powells_trick=False)\n",
    "\n",
    "# Only Powell's trick\n",
    "ex7_eq_sqp(with_line_search=False, with_powells_trick=True)\n",
    "\n",
    "# Linesearch and Powell's trick\n",
    "ex7_eq_sqp(with_line_search=True, with_powells_trick=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f44f0a7-ca3b-4839-8bf8-84710f22b0a6",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Consider the inequality constrained problem:\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "\\min_x & \\quad f(x) \\\\\n",
    "\\text{s.t.} & \\quad h(x) = 0 \\\\\n",
    "     & \\quad g(x) \\leq 0\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "with $f\\colon\\R^n\\to\\R$, $h\\colon\\R^n\\to\\R^m$, and $g\\colon\\R^n\\to\\R^l$.\n",
    "Introduce Lagrange multipliers $\\lambda\\in\\R^m$ and $\\mu\\in\\R^l$ to handle the constraints:\n",
    "\\begin{align*}\n",
    "{L}(x,\\lambda,\\mu) = f(x) + \\lambda^T h(x) + \\mu^T g(x).\n",
    "\\end{align*}\n",
    "The optimality conditions are:\n",
    "\\begin{align*}\n",
    " 0 &= \\nabla f(x) + \\nabla h(x) \\lambda + \\nabla g(x) \\mu \\\\\n",
    " 0 &= h(x) \\\\\n",
    " 0 &\\geq g(x) \\\\\n",
    " 0 &\\leq \\mu \\\\\n",
    " 0 &= \\mu^T g(x).\n",
    "\\end{align*}\n",
    "In the equality-constrained case we linearized and solved the optimality conditions either by solving the linear system of equations by hand, or by solving the equivalent QP with a QP solver.\n",
    "It is a pain to solve something with inequalities by hand, so here we let the QP solver do the work.\n",
    "The equivalent QP is:\n",
    "\\begin{align}\n",
    "\\begin{split}\n",
    "\\min_{\\Delta x} & \\ \\Delta x^T\\nabla f_k + \\frac{1}{2}\\Delta x^T \\nabla_{xx}^2{L}_k\\Delta x \\\\\n",
    "\\text{s.t.} & \\ \\nabla h_k^T \\Delta x + h_k = 0 \\\\\n",
    "              & \\ \\nabla g_k^T \\Delta x + g_k \\leq 0 \\\\\n",
    "\\end{split}\n",
    "\\end{align}\n",
    "\n",
    "### Exercise 2.a\n",
    "Add inequality constraints to your SQP solver by completing the code below.\n",
    "You will need to get $\\lambda$ and $\\mu$ from the output of conic.\n",
    " Use BFGS with Powell's trick to approximate the Hessian $\\nabla_{xx}^2 {L}$.\n",
    "For the line search merit function, use:\n",
    "\\begin{align*}\n",
    "\\varphi_1(x) := f(x) + c \\left( \\|h(x)\\|_1 + \\|g^{+}(x)\\|_1 \\right)\n",
    "%\\label{eq:meritFunction}\n",
    "\\end{align*}\n",
    "and\n",
    "\\begin{align*}\n",
    "\\nabla \\varphi_1(x_k)^Td_k = \\nabla f(x_k)^Td_k - c \\left( \\|h(x_k)\\|_1 + \\|g^{+}(x_k)\\|_1 \\right),\n",
    "%\\label{eq:meritGradient}\n",
    "\\end{align*}\n",
    "where\n",
    "\\begin{align*}\n",
    "g^+(x) = \\max(0,g(x)) \\qquad \\text{ (pointwise)}.\n",
    "\\end{align*}\n",
    "\n",
    "Use as stopping criterion:\n",
    "\\begin{align*}\n",
    "\\left\\|\n",
    "\\nabla f_k + \\nabla h_k \\lambda_k + \\nabla g_k\\mu_k\n",
    "\\right\\|_\\infty\n",
    " \\leq \\epsilon  \\\\\n",
    "\\end{align*}\n",
    "and\n",
    "\\begin{align*}\r\n",
    "\\left\\|\r\n",
    "\\begin{pmatrix}\r\n",
    "h_k \\\\\r\n",
    "g_k^+\r\n",
    "\\end{pmatrix}\r\n",
    "\\right\\|_\\infty\r\n",
    " \\leq \\epsilon.\r\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fc978b-cf20-432e-9ea8-48f7d79a0460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_sqp(ffun, hfun, gfun, x0: np.array, with_line_search=True, with_powells_trick=False, callback=None):\n",
    "\n",
    "    # convergence tolerance\n",
    "    convergence_tol = 1e-4\n",
    "    max_iters = 1000\n",
    "    \n",
    "    # make sure x0 is a column vector\n",
    "    n_variables, cols = x0.shape\n",
    "    if cols != 1:\n",
    "        raise ValueError('x0 needs to be a column vector')\n",
    "    \n",
    "    # make sure ffun returns a scalar\n",
    "    f0 = ffun(x0)\n",
    "    if len(f0) != 1:\n",
    "        raise ValueError('ffun must return a scalar')\n",
    "    \n",
    "    # make sure hfun returns a column vector\n",
    "    h0 = hfun(x0)\n",
    "    n_eq_constraints, cols = h0.shape\n",
    "    if cols != 1:\n",
    "        raise ValueError('hfun needs to return a column vector')\n",
    "    \n",
    "    # make sure gfun returns a column vector\n",
    "    g0 = gfun(x0)\n",
    "    n_ineq_constraints, cols = g0.shape\n",
    "    if cols != 1:\n",
    "        raise ValueError('gfun needs to return a column vector')\n",
    "    \n",
    "    n_constraints = n_eq_constraints + n_ineq_constraints\n",
    "    # a log of the iterations\n",
    "    x_iters = np.zeros((n_variables, max_iters))\n",
    "    gradient_lagrangian_iters = np.zeros((1,max_iters))\n",
    "    \n",
    "    # initial variables\n",
    "    x      = x0.copy()\n",
    "    lambdA = np.zeros((n_eq_constraints, 1)) # lambda is a key word\n",
    "    mu     = np.zeros((n_ineq_constraints, 1))\n",
    "    \n",
    "    # set these so the first printout doesn't fail\n",
    "    dk     = np.zeros((n_variables, 1))\n",
    "    alpha = 0\n",
    "    qp_status = \"default\"\n",
    "    \n",
    "    # initialize B\n",
    "    B = np.eye(n_variables)\n",
    "    \n",
    "    # evaluate the initial f and it's jacobian J\n",
    "    _, J = finite_difference_jacobian(ffun, x)\n",
    "    \n",
    "    # evaluate the initial h and it's jacobian J_h\n",
    "    h, J_h = finite_difference_jacobian(hfun, x)\n",
    "    \n",
    "    # evaluate the initial g and it's jacobian J_g\n",
    "    g, J_g = finite_difference_jacobian(gfun, x)\n",
    "    \n",
    "    # Define the QP solver with Casadi\n",
    "    B_placeholder = cs.DM.ones(n_variables, n_variables)\n",
    "    Jh_placeholder = cs.DM.ones(n_constraints, n_variables)\n",
    "    qp_struct = {   'h': B_placeholder.sparsity(),\n",
    "                    'a': Jh_placeholder.sparsity()}\n",
    "    \n",
    "\n",
    "    qp_solver_opts = {}\n",
    "    qp_solver_opts[\"print_out\"] = False\n",
    "    qp_solver_opts[\"printLevel\"] = \"none\"\n",
    "\n",
    "    qp_solver = cs.conic(\"qpsol\", \"qpoases\", qp_struct, qp_solver_opts)\n",
    "\n",
    "    # Main optimization loop\n",
    "    for k in range(max_iters):\n",
    "        \n",
    "        # check for divergence\n",
    "        x_norm_inf = np.linalg.norm(x, np.inf)\n",
    "        if x_norm_inf > 1e6:\n",
    "            raise ValueError('minimize_sqp has diverged, ||x||_\\{inf\\}: %.3g',x_norm_inf)\n",
    "        \n",
    "        # check for bad BFGS update\n",
    "        if np.any(np.any(np.isnan(B))):\n",
    "            raise ValueError('BFGS has NaNs in it, step size is probably very small')\n",
    "        \n",
    "        # store x in the iteration log\n",
    "        gradient_lagrangian = J.T + J_h.T  @ lambdA + J_g.T @ mu\n",
    "        norm_gradient_lagrangian = np.linalg.norm(gradient_lagrangian, np.inf)\n",
    "        norm_infeasbility = np.linalg.norm( np.vstack((h, np.fmax(g,0))), np.inf)\n",
    "    \n",
    "        norm_convergence = np.fmax(norm_gradient_lagrangian, norm_infeasbility)\n",
    "\n",
    "        x_iters[:,[k]] = x.copy()\n",
    "        gradient_lagrangian_iters[0, k] = norm_convergence\n",
    "        \n",
    "        # Iteration output\n",
    "        if k % 10 == 0:\n",
    "            print('{0: >8}{1: >20}{2: >20}{3: >30}{4: >15}{5: >7}'.format(\"iter\", \"qpstatus\", \"||grad_L||\", \"||infeasibility_measure||\", \"||step||\", \"t\"))\n",
    "        print('{0: >8d}{1: >20}{2: >20.4e}{3: >30.4e}{4: >15.4e}{5: >7.4f}'.format(k, qp_status, norm_gradient_lagrangian, norm_infeasbility, np.linalg.norm(dk,np.inf), alpha))\n",
    "        \n",
    "        if callback is not None:\n",
    "            callback(x)\n",
    "\n",
    "        # check for convergence\n",
    "        if norm_convergence < convergence_tol:\n",
    "            x_iters = x_iters[:,:k+1]\n",
    "            gradient_lagrangian_iters = gradient_lagrangian_iters[:,:k+1]\n",
    "            print('acceptable solution found\\n')\n",
    "            return x, x_iters, gradient_lagrangian_iters\n",
    "        \n",
    "        x_old = x.copy()\n",
    "        \n",
    "        #################### FILL THIS PART IN ########################\n",
    "        # find the search direction and lambda\n",
    "        lba = cs.vertcat(-h, -cs.inf*cs.DM.ones(n_ineq_constraints))\n",
    "        uba = cs.vertcat(-h, -g)\n",
    "        \n",
    "        result = qp_solver(h=B,\n",
    "                           a=cs.vertcat(J_h, J_g),\n",
    "                           g=J,\n",
    "                           lba=lba,\n",
    "                           uba=uba)\n",
    "\n",
    "        dk = np.array(result['x'])\n",
    "        lambdA = np.array(result['lam_a'])[:n_eq_constraints]\n",
    "        mu = np.array(result['lam_a'])[n_eq_constraints:]\n",
    "        qp_status = qp_solver.stats()['return_status']\n",
    "    \n",
    "        ### END OF DIFFERENT APPROACHES ---------------------------------------\n",
    "        # take the line search\n",
    "        if with_line_search:\n",
    "            sigma = 0.01\n",
    "            beta  = 0.6\n",
    "            c     = 100\n",
    "            # calculate the directional derivative\n",
    "            directional_derivative = J @ dk - c * (np.linalg.norm(h,1) + np.linalg.norm(np.fmax(g,0),1))\n",
    "            merit_fun = lambda y : merit_function_inequality_constrained_sqp(ffun, hfun, gfun, c, y)\n",
    "            x, alpha = line_search(merit_fun, x, directional_derivative, dk, sigma, beta)\n",
    "        else:\n",
    "            x += dk\n",
    "\n",
    "        # update BFGS hessian approximation\n",
    "        J_old   = J.copy()\n",
    "        J_h_old = J_h.copy()\n",
    "        J_g_old = J_g.copy()\n",
    "        \n",
    "        # evaluate F and it's jacobian J\n",
    "        _, J = finite_difference_jacobian(ffun, x)\n",
    "    \n",
    "        # evaluate h and it's jacobian J_h\n",
    "        h, J_h = finite_difference_jacobian(hfun, x)\n",
    "        \n",
    "        # evaluate g and it's jacobian J_g\n",
    "        g, J_g = finite_difference_jacobian(gfun, x)\n",
    "        \n",
    "        Lx_old = J_old.T + J_h_old.T  @ lambdA + J_g_old.T @ mu\n",
    "        Lx     = J.T + J_h.T  @ lambdA + J_g.T @ mu\n",
    "        \n",
    "        s = x - x_old\n",
    "        y = Lx - Lx_old\n",
    "\n",
    "        # Powell's trick\n",
    "        if with_powells_trick:\n",
    "            if y.T @ s >= 0.2 * s.T @ B @ s:\n",
    "                theta = 1\n",
    "            else:\n",
    "                theta = (0.8 * s.T @ B @ s)/(s.T @ B @ s -s.T @ y)\n",
    "\n",
    "            y = theta*y + (1-theta) * B @ s\n",
    "        else:\n",
    "            y = y\n",
    "        \n",
    "        B = B - B@(s@s.T)@B/(s.T @B@s) + y@y.T/(s.T@y)\n",
    "        B = (B + B.T) / 2 # Symmetrize Hessian, only needed due to numerical inaccuracy, so that quadprog doesn't complain\n",
    "        ###############################################################\n",
    "    \n",
    "    raise RuntimeError('minimize_sqp: max iterations exceeded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913b5bfa-26ab-4004-ae8e-5c102eddc9b5",
   "metadata": {},
   "source": [
    "### Exercise 2.b\n",
    "Verify your modification by solving\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "\\min_{x,y} & \\ \\frac{1}{2}\\left( x^2 + \\left(\\frac{y}{2}\\right)^2 \\right) \\\\\n",
    "\\text{s.t.} & \\ y = (x-1)^2 - x + 3 \\\\\n",
    "              & \\ 0 \\leq 2x - 0.4x^2 - y\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "\n",
    "To do this, we define the inequality constraint function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0c1d3f-eadb-4b1c-8dfb-a16160bf814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inequality constraint function\n",
    "def gfun(xy):\n",
    "    # such that g(x) <= 0\n",
    "    x = xy[0]\n",
    "    y = xy[1]\n",
    "    \n",
    "    g = -2.0*x + 0.4*x**2 + y\n",
    "    return np.array([g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0981ed9-b256-46f7-b426-005dd0762f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex7_sqp(with_line_search=True, with_powells_trick=False, plot_function=False):\n",
    "\n",
    "    # ------ First let's plot the function ------\n",
    "    # Define the the meshgrids for the 2D function\n",
    "    xlist = np.linspace(-1, 4, 40)\n",
    "    ylist = np.linspace(-3, 4, 40)\n",
    "    [X,Y] = np.meshgrid(xlist, ylist)\n",
    "    obj = np.vectorize(lambda x, y: ffun([x, y]), signature=\"(),()->()\")(X, Y)\n",
    "\n",
    "    if plot_function:\n",
    "        # Plot contour lines\n",
    "        fig =plt.figure()\n",
    "        c = plt.contour(X, Y, obj, 40)\n",
    "        \n",
    "        t = np.linspace(-4,4,400)\n",
    "        plt.plot(t, 3 + (t-1)**2 - t, color='black', label='equality constraint')\n",
    "        plt.plot(t, 2*t - 0.4*t**2,color='black', label='inequality constraint', linestyle=\"dashed\")\n",
    "    \n",
    "        plt.title('Contour plot of objective function with constraint')\n",
    "        plt.xlabel('x [cm]')\n",
    "        plt.ylabel('y [cm]')\n",
    "        plt.xlim((-1, 4))\n",
    "        plt.ylim((-3, 4))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # ------ Solve the problem ------\n",
    "    x0 = np.array([[3.0],[1.0]])\n",
    "\n",
    "    _, x, gradients_lagrangian = minimize_sqp(ffun, hfun, gfun, x0, with_line_search=with_line_search, with_powells_trick=with_powells_trick)\n",
    "\n",
    "    n_iter = gradients_lagrangian.shape[1]\n",
    "\n",
    "    # # ------ Plot the solution ------\n",
    "    # Plot iterates x\n",
    "    # Plot contour lines\n",
    "    fig = plt.figure()\n",
    "    c = plt.contour(X, Y, obj)\n",
    "    t = np.linspace(-4,4,400)\n",
    "    plt.plot(t, 3 + (t-1)**2 - t, color='black', label='equality constraint')\n",
    "    plt.plot(t, 2*t - 0.4*t**2,color='black', label='inequality constraint', linestyle=\"dashed\")\n",
    "\n",
    "    plt.plot(x[0,:], x[1,:],color='r')\n",
    "    plt.plot(x[0,:], x[1,:],color='r', marker=\"o\")\n",
    "    plt.title('Contour plot of objective function with constraint')\n",
    "    plt.xlabel('x [cm]')\n",
    "    plt.ylabel('y [cm]')\n",
    "    plt.xlim((-1, 4))\n",
    "    plt.ylim((-3, 4))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot convergence\n",
    "    plot_convergence(n_iter, gradients_lagrangian)\n",
    "\n",
    "ex7_sqp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027e06e7-e0c6-45b3-9a1f-642d43a9470c",
   "metadata": {},
   "source": [
    "### Exercise 2.c\n",
    "Try solving the problem with both line search and full step. What is the\n",
    "difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b889a89d-eb21-4bff-bb9b-a6a4d4694f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No linesearch, no Powell\n",
    "ex7_sqp(with_line_search=False, with_powells_trick=False)\n",
    "\n",
    "# Only linesearch\n",
    "ex7_sqp(with_line_search=True, with_powells_trick=False)\n",
    "\n",
    "# Only Powell's trick\n",
    "ex7_sqp(with_line_search=False, with_powells_trick=True)\n",
    "\n",
    "# Linesearch and Powell's trick\n",
    "ex7_sqp(with_line_search=True, with_powells_trick=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89945b8d-f8b4-4b07-91b7-5864b06ce036",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "We will solve the hanging chain problem. In this case we will consider a completely inelastic chain where the distance between links is fixed, and the chain is partially resting on an inclined table with slope $0.15$:\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "\\min_{x_1..x_N,y_1..y_N} & \\ \\sum_{k=1}^N y_k  \\\\\n",
    "\\text{s.t.} & \\ x_1 = -1  \\\\\n",
    "              & \\ x_N = 1  \\\\\n",
    "              & \\ y_1 = 1  \\\\\n",
    "              & \\ y_N = 1  \\\\\n",
    "              & \\ (x_{k+1} - x_k)^2 + (y_{k+1} - y_k)^2 = r^2, \\,\\,\\,\\,\\, k=1 \\dots N-1 \\\\\n",
    "              & \\ y_k \\geq 0.15 x_k + 0.3, \\,\\,\\,\\,\\, k=1 \\dots N\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "with $r=1.4 \\cdot 2/N$.\n",
    "\n",
    "### Exercise 3.a\n",
    "Complete the code below to solve this problem.\n",
    "Start by trying to solve this for small $N$, and see how large you can make $N$.\n",
    "Be extremely careful about the initial guess. You can use as initial guess \n",
    "x0=np.linspace(-1,1,N), y0=np.ones(1,N)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9d584e-6832-427c-92e4-79ca040dbf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffun(xy: np.array):\n",
    "    N = round(xy.shape[0]/2)\n",
    "    x = xy[:N]\n",
    "    y = xy[N:]\n",
    "    \n",
    "    V = np.sum(y)\n",
    "    return np.array([[V]])\n",
    "\n",
    "def hfun(xy):\n",
    "    N = round(xy.shape[0]/2)\n",
    "    x = xy[:N]\n",
    "    y = xy[N:]\n",
    "    \n",
    "    r = 1.4*2/N\n",
    "    \n",
    "    h = (x[1:]-x[0:-1])**2 + (y[1:]-y[0:-1])**2 - r**2\n",
    "    h = np.vstack((h,\n",
    "                   x[0]   + 1,\n",
    "                   x[-1] - 1,\n",
    "                   y[0]   - 1,\n",
    "                   y[-1] - 1))\n",
    "    return h\n",
    "\n",
    "def gfun_linear(xy):\n",
    "    N = round(xy.shape[0]/2)\n",
    "    x = xy[:N]\n",
    "    y = xy[N:]\n",
    "    \n",
    "    g = 0.15*x + 0.3 - y             # convex\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e2f7d-47d9-455c-b268-ef9c4d7d91a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex7_sqp_chain(use_linear_constraints=True):\n",
    "    N = 20\n",
    "\n",
    "    x0 = np.linspace(-1,1,N)\n",
    "\n",
    "    # Different initial guesses for y\n",
    "    # y0 = np.ones(N)\n",
    "    y0 = 1+0.2*np.cos(x0*np.pi/2)\n",
    "\n",
    "    if use_linear_constraints:\n",
    "        gfun = gfun_linear\n",
    "    else:\n",
    "        gfun = gfun_quadratic\n",
    "\n",
    "    xy0 = np.vstack((x0.reshape((N,1)), y0.reshape((N,1))))\n",
    "\n",
    "    [xy ,_, gradients_lagrangian] = minimize_sqp(ffun, hfun, gfun, xy0, with_line_search=True, with_powells_trick=True, callback=callback)\n",
    "    x = xy[:N]\n",
    "    y = xy[N:]\n",
    "    n_iter = gradients_lagrangian.shape[1]\n",
    "\n",
    "    # # ------ Plot the solution ------\n",
    "    # plot solution\n",
    "    fig1 = plt.figure()\n",
    "    plt.plot(x, y, 'bo', label='chain elements')\n",
    "    plt.plot(x, y, 'r', label='chain')\n",
    "\n",
    "    if use_linear_constraints:\n",
    "        plt.plot(x, 0.15*x+0.3, color='k', label=\"linear constraint\")\n",
    "    else:\n",
    "        plt.plot(x, -0.6*x**2 +0.15*x +0.5, color='k', label='quadratic constraint')\n",
    "\n",
    "    title_str = \"Optimal solution\"\n",
    "    plt.title(title_str)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot convergence\n",
    "    plot_convergence(n_iter, gradients_lagrangian)\n",
    "\n",
    "def callback(x):\n",
    "    N = round(x.shape[0]/2)\n",
    "    plt.plot(x[0:N], x[N:],'r')\n",
    "    plt.plot(x[0:N], x[N:],'bo')\n",
    "    plt.title(\"SQP Iterations\")    \n",
    "\n",
    "ex7_sqp_chain(use_linear_constraints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70da5dc4-fdad-4af9-a99c-cf19ec517cf3",
   "metadata": {},
   "source": [
    "### Exercise 3.b\n",
    "In the code above, try different initial guesses like y0=1+0.2*np.cos(x0*pi/2). Can you explain\n",
    "this behavior?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea12dbd-71f4-4630-9867-c281333d5097",
   "metadata": {},
   "source": [
    "### Exercise 3.a\n",
    "Remove the convex constraint\r\n",
    "\\begin{align*}\r\n",
    "y_k \\geq 0.15x_k + 0.3, \\,\\,\\,\\,\\, k=1 \\dots N\r\n",
    "\\end{align*}\r\n",
    "\r\n",
    "and add the non-convex constraint\r\n",
    "\r\n",
    "\\begin{align*}\r\n",
    "y_k \\geq -0.6x_k^2 + 0.15x_k + 0.5, \\,\\,\\,\\,\\, k=1 \\dots N\r\n",
    "\\end{align*}\r\n",
    "\r\n",
    "and solve the pro Comment on the results.lem again.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86548657-b65f-4a2e-b38a-5cf5cc3ecf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gfun_quadratic(xy):\n",
    "    N = round(xy.shape[0]/2)\n",
    "    x = xy[:N]\n",
    "    y = xy[N:]\n",
    "    \n",
    "    g = -0.6*x**2 + 0.15*x + 0.5 - y # non-convex\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0141df1d-3483-45a6-854c-952772feeff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex7_sqp_chain(use_linear_constraints=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b182897d-3b04-4cdd-8a19-ebb7671d2d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
